{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanmay Suhas Jagtap\n",
    "#### Portfolio : https://78-t0b1.github.io/Portfolio.github.io/\n",
    "#### Linkein : https://www.linkedin.com/in/tanmay-jagtap-t0b1/\n",
    "#### Github : https://github.com/78-t0b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 1**\n",
    "\n",
    "## What is NLP?\n",
    "\n",
    "[IBM](https://www.ibm.com/topics/natural-language-processing)\n",
    "\n",
    "Natural language processing (NLP) is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language. \n",
    "\n",
    "## What are its applications?\n",
    "\n",
    "Finance: In financial dealings, nanoseconds might make the difference between success and failure when accessing data, or making trades or deals. NLP can speed the mining of information from financial statements, annual and regulatory reports, news releases or even social media.\n",
    "\n",
    "Healthcare: New medical insights and breakthroughs can arrive faster than many healthcare professionals can keep up. NLP and AI-based tools can help speed the analysis of health records and medical research papers, making better-informed medical decisions possible, or assisting in the detection or even prevention of medical conditions.\n",
    "\n",
    "Insurance: NLP can analyze claims to look for patterns that can identify areas of concern and find inefficiencies in claims processing—leading to greater optimization of processing and employee efforts.\n",
    "\n",
    "Legal: Almost any legal case might require reviewing mounds of paperwork, background information and legal precedent. NLP can help automate legal discovery, assisting in the organization of information, speeding review and helping ensure that all relevant details are captured for consideration.\n",
    "\n",
    "\n",
    "**Types of NLP business applications :**\n",
    "[twilio blog](https://www.twilio.com/en-us/blog/nlp-steps)\n",
    "\n",
    "- Text to speech: Converting text-to-speech data, then reproducing the text as natural-sounding speech\n",
    "- Chatbots: Helping chatbots understand and respond to customer inquiries\n",
    "- Urgency detection: Analyzing language to prioritize tasks\n",
    "- Natural language understanding: Converting speech to text and analyzing its intent\n",
    "- Autocorrect: Detecting and removing text errors and suggesting alternatives\n",
    "- Sentiment analysis: Revealing the perceptions people have of your goods and services and those of your competitors\n",
    "- Speech recognition: Powering applications that understand users’ voices and deciphering their meaning\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecycle of NLP project\n",
    "\n",
    "\n",
    "[Neptune AI blog](https://neptune.ai/blog/life-cycle-of-a-machine-learning-project)\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "1. Problem Understanding\n",
    "- Clearly define the objectives and requirements of the NLP project, such as whether it will be integrated into a chatbot, used for a specific industry, or designed as a generic model.\n",
    "- Understand the context and input types the NLP model needs to handle, such as emojis, titles, and conversational context.\n",
    "- Clarify the purpose and intended use case of the sentiment analysis or other NLP capabilities.\n",
    "\n",
    "2. Data Collection and Labeling\n",
    "- Determine if the client has provided any initial data, or if you need to start from scratch with data collection.\n",
    "- Explore techniques to handle limited data, such as using GPT models for synthetic data generation or leveraging data labeling tools like Argilla.\n",
    "- Invest significant time and resources into high-quality data collection and annotation, as this is crucial for building an effective NLP model.\n",
    "\n",
    "3. Data Preparation and Preprocessing\n",
    "- Perform lexical analysis to segment text into meaningful units like words, phrases, and sentences.\n",
    "- Conduct syntactic analysis to check the grammatical structure of the text.\n",
    "- Carry out semantic analysis to understand the meaning and context of the text.\n",
    "- Handle other preprocessing steps like stop word removal, stemming/lemmatization, and feature engineering.\n",
    "\n",
    "4. Model Development and Training\n",
    "- Select appropriate NLP models and techniques, such as traditional machine learning algorithms or newer transformer-based models like BERT.\n",
    "- Train the NLP models using the prepared dataset, optimizing hyperparameters and monitoring performance.\n",
    "- Evaluate the model's performance using relevant metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "5. Model Deployment and Maintenance\n",
    "- Integrate the trained NLP model into the production environment, whether that's a chatbot, an analytics system, or some other application.\n",
    "- Monitor the model's performance in the real-world setting and fine-tune or retrain it as needed to maintain high accuracy.\n",
    "- Ensure the NLP system can handle evolving language patterns, new data sources, and changing user requirements over time.\n",
    "\n",
    "\n",
    "Throughout this lifecycle, it's important to maintain close collaboration between domain experts, data scientists, and software engineers to ensure the NLP project meets the desired business objectives. The iterative nature of NLP projects also requires flexibility and a willingness to revisit earlier stages as needed to improve the model's performance and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Understanding\n",
    "\n",
    "Understanding the different algorithms and their applications in Natural Language Processing (NLP) is crucial for solving various text-based problems. Here’s a detailed breakdown of common NLP tasks, the algorithms used for each task, and their applications:\n",
    "\n",
    "### 1. Sentiment Analysis\n",
    "**Purpose**: Determine the sentiment (positive, negative, neutral) expressed in text.\n",
    "\n",
    "**Common Algorithms**:\n",
    "- **Lexicon-Based Methods**: Use predefined dictionaries of words associated with positive or negative sentiments. Examples include VADER (Valence Aware Dictionary and sEntiment Reasoner) and SentiWordNet.\n",
    "  - **When to Use**: Quick and interpretable sentiment analysis on relatively simple texts where context and sarcasm are not major concerns.\n",
    "  \n",
    "- **Machine Learning Classifiers**: Use algorithms like Naive Bayes, Support Vector Machines (SVM), or Logistic Regression trained on labeled datasets.\n",
    "  - **When to Use**: When you have labeled training data and need more flexibility than lexicon-based methods.\n",
    "  \n",
    "- **Deep Learning Models**: Use Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and transformer-based models like BERT for more accurate sentiment analysis.\n",
    "  - **When to Use**: When dealing with complex texts where context, syntax, and semantics play a significant role.\n",
    "\n",
    "### 2. Named Entity Recognition (NER)\n",
    "**Purpose**: Identify and classify named entities (people, organizations, locations, etc.) in text.\n",
    "\n",
    "**Common Algorithms**:\n",
    "- **Conditional Random Fields (CRFs)**: Probabilistic models used for structured prediction.\n",
    "  - **When to Use**: Traditional approach, effective for many NER tasks with structured and labeled training data.\n",
    "  \n",
    "- **Neural Networks**: Use architectures like BiLSTM-CRF to combine neural networks with CRFs for improved performance.\n",
    "  - **When to Use**: When you need more sophisticated models that can capture complex dependencies in the text.\n",
    "  \n",
    "- **Transformer Models**: Models like BERT, RoBERTa, and GPT-3 fine-tuned for NER tasks.\n",
    "  - **When to Use**: When state-of-the-art performance is required, particularly for texts with rich contextual information.\n",
    "\n",
    "### 3. Text Classification\n",
    "**Purpose**: Assign predefined categories or labels to text.\n",
    "\n",
    "**Common Algorithms**:\n",
    "- **Naive Bayes**: Probabilistic classifier based on Bayes’ theorem.\n",
    "  - **When to Use**: Simple, fast, and effective for many text classification tasks, especially with small datasets.\n",
    "  \n",
    "- **Support Vector Machines (SVM)**: Supervised learning models that can perform linear and non-linear classification.\n",
    "  - **When to Use**: When you need robust performance on medium-sized datasets and can afford the computational cost.\n",
    "  \n",
    "- **Convolutional Neural Networks (CNNs)**: Capture local patterns in text, often used for short text classification.\n",
    "  - **When to Use**: When classifying text that benefits from capturing spatial hierarchies (e.g., sentiment in sentences).\n",
    "  \n",
    "- **Transformers**: BERT, DistilBERT, and other transformer models fine-tuned for text classification.\n",
    "  - **When to Use**: For high performance on complex classification tasks with sufficient computational resources.\n",
    "\n",
    "### 4. Machine Translation\n",
    "**Purpose**: Translate text from one language to another.\n",
    "\n",
    "**Common Algorithms**:\n",
    "- **Statistical Machine Translation (SMT)**: Uses statistical models based on the analysis of bilingual text corpora.\n",
    "  - **When to Use**: Older approach, less common now, suitable when limited computational resources are available.\n",
    "  \n",
    "- **Neural Machine Translation (NMT)**: Uses neural networks to model the entire translation process.\n",
    "  - **When to Use**: When you have sufficient data and computational power to train neural models.\n",
    "  \n",
    "- **Transformer-Based Models**: Use models like Transformer, BERT, and GPT for translation tasks.\n",
    "  - **When to Use**: For state-of-the-art translation performance, particularly in complex and high-resource language pairs.\n",
    "\n",
    "### 5. Summarization\n",
    "**Purpose**: Generate concise summaries of longer text.\n",
    "\n",
    "**Common Algorithms**:\n",
    "- **Extractive Summarization**: Selects key sentences from the original text to form the summary. Algorithms include TextRank and other graph-based methods.\n",
    "  - **When to Use**: When you need quick, interpretable summaries that preserve the original text’s meaning.\n",
    "  \n",
    "- **Abstractive Summarization**: Generates summaries by creating new sentences, using models like seq2seq (sequence-to-sequence) with attention mechanisms, and T5 (Text-to-Text Transfer Transformer).\n",
    "  - **When to Use**: When you need more natural and human-like summaries that may rephrase or paraphrase the original text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preparation and Preprocessing\n",
    "\n",
    "In NLP data preperation in diffrent than other datsets. Main reson is data we are handeling here is unstructured. Text pre-processing is the process of transforming unstructured text to structured text to prepare it for analysis. To understand text pre processing first lets understand what does word means.\n",
    "\n",
    "### Word\n",
    "\n",
    "[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf)\n",
    "\n",
    " **_\"He stepped out into the hall, was delighted to encounter a water brother.\"_**\n",
    "\n",
    "This sentence has 13 words if we don’t count punctuation marks as words, 15\n",
    "if we count punctuation. Whether we treat period (“.”), comma (“,”), and so on as\n",
    "words depends on the task. Punctuation is critical for finding boundaries of things\n",
    "(commas, periods, colons) and for identifying some aspects of meaning (question\n",
    "marks, exclamation marks, quotation marks). For some tasks, like part-of-speech\n",
    "tagging or parsing or speech synthesis, we sometimes treat punctuation marks as if\n",
    "they were separate words.\n",
    "\n",
    "**_\"They picnicked by the pool, then lay back on the grass and looked at the stars.\"_**\n",
    "\n",
    "Should we consider a capitalized string (like They) and one that is uncapitalized (like they) to be the same word type?\n",
    "The answer is that it depends on the task! They and they might be lumped together\n",
    "as the same type in some tasks, like speech recognition, where we might just care\n",
    "about getting the words in order and don’t care about the formatting, while for other\n",
    "tasks, such as deciding whether a particular word is a noun or verb (part-of-speech\n",
    "tagging) or whether a word is a name of a person or location (named-entity tagging), capitalization is a useful feature and is retained. Sometimes we keep around\n",
    "two versions of a particular NLP model, one with capitalization and one without\n",
    "capitalization.\n",
    "\n",
    "Because language is so situated, when developing computational models for language processing from a corpus, it’s important to consider who produced the language, in what context, for what purpose. How can a user of a dataset know all these\n",
    "details? The best way is for the corpus creator to build a datasheet (Gebru et al.,\n",
    "2020) or data statement (Bender et al., 2021) for each corpus. A datasheet specifies\n",
    "properties of a dataset like:\n",
    "\n",
    "Motivation: Why was the corpus collected, by whom, and who funded it?\n",
    "\n",
    "Situation: When and in what situation was the text written/spoken? For example,\n",
    "was there a task? Was the language originally spoken conversation, edited\n",
    "text, social media communication, monologue vs. dialogue?\n",
    "\n",
    "Language variety: What language (including dialect/region) was the corpus in?\n",
    "\n",
    "Speaker demographics: What was, e.g., the age or gender of the text’s authors?\n",
    "\n",
    "Collection process: How big is the data? If it is a subsample how was it sampled?\n",
    "Was the data collected with consent? How was the data pre-processed, and\n",
    "what metadata is available?\n",
    "\n",
    "Annotation process: What are the annotations, what are the demographics of the\n",
    "annotators, how were they trained, how was the data annotated?\n",
    "\n",
    "Distribution: Are there copyright or other intellectual property restrictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "At least three tasks are commonly applied as part of any normalization process:\n",
    "\n",
    "1. Tokenizing (segmenting) words\n",
    "2. Normalizing word formats\n",
    "3. Segmenting sentences\n",
    "\n",
    "## 1. Tokenization\n",
    "\n",
    "**_\"Tokenization is the process of breaking down a piece of text, like a sentence or a paragraph, into individual words or “tokens.” These tokens are the basic building blocks of language, and tokenization helps computers understand and process human language by splitting it into manageable units.\"_**\n",
    "\n",
    "There are roughly two classes of tokenization\n",
    "algorithms. \n",
    "\n",
    "In top-down tokenization, we define a standard and implement rules\n",
    "to implement that kind of tokenization.\n",
    "\n",
    "In bottom-up tokenization, we use simple\n",
    "statistics of letter sequences to break up words into subword tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top down Tokenization:\n",
    "\n",
    "Top-down (rule-based) tokenization is a method for dividing text into smaller components (tokens) using predefined rules. This approach uses a set of deterministic rules to identify boundaries between tokens, such as words, punctuation marks, and other meaningful elements. \n",
    "\n",
    "Key Features of Top-Down (Rule-Based) Tokenization:\n",
    "1. Deterministic: The rules are fixed and do not change, leading to predictable results.\n",
    "2. Predefined Rules: These rules are usually handcrafted by linguists or experts in the language.\n",
    "3. Simplicity: Often simpler and faster than statistical methods since they do not rely on training data.\n",
    "\n",
    "Example: \"Email me at user@example.com\" → [\"Email\", \"me\", \"at\", \"user\", \"@\", \"example\", \".\", \"com\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', \"It's\", 'a', 'well', 'known', 'fact', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    # Define the tokenization pattern\n",
    "    pattern = r\"[\\w']+|[.,!?;]\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, world! It's a well-known fact.\"\n",
    "tokens = simple_tokenizer(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottom-up Tokenization:\n",
    "\n",
    "Instead of defining tokens as words (whether delimited by spaces\n",
    "or more complex algorithms), or as characters (as in Chinese), we can use our data to\n",
    "automatically tell us what the tokens should be. This is especially useful in dealing\n",
    "with unknown words, an important problem in language processing. As we will\n",
    "see in the next chapter, NLP algorithms often learn some facts about language from\n",
    "one corpus (a training corpus) and then use these facts to make decisions about a\n",
    "separate test corpus and its language. Thus if our training corpus contains, say the\n",
    "words low, new, newer, but not lower, then if the word lower appears in our test\n",
    "corpus, our system will not know what to do with it.\n",
    "\n",
    "To deal with this unknown word problem, modern tokenizers automatically insubwords duce sets of tokens that include tokens smaller than words, called subwords. Subwords can be arbitrary substrings, or they can be meaning-bearing units like the\n",
    "morphemes -est or -er. (A morpheme is the smallest meaning-bearing unit of a language; for example the word unlikeliest has the morphemes un-, likely, and -est.)\n",
    "\n",
    "In modern tokenization schemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er. Every unseen word like\n",
    "lower can thus be represented by some sequence of known subword units, such as\n",
    "low and er, or even as a sequence of individual letters if necessary.\n",
    "\n",
    "##### **Byte-pair Encoding**\n",
    "\n",
    "BPE starts with the smallest possible units (individual characters) and iteratively merges the most frequent pairs of symbols to form larger units. This process continues until a predefined vocabulary size is reached.\n",
    "\n",
    "Steps of Byte Pair Encoding\n",
    "1. Initialize Vocabulary: Start with a vocabulary that includes all individual characters in the text corpus.\n",
    "2. Count Pairs: Count all adjacent character pairs in the corpus.\n",
    "3. Merge the Most Frequent Pair: Find the most frequent pair of characters and merge them into a new symbol.\n",
    "4. Repeat: Repeat the process of counting and merging until the desired vocabulary size is achieved.\n",
    "\n",
    "Byte Pair Encoding is a powerful tokenization method widely used in NLP, especially in models like GPT and BERT. It balances the need for a manageable vocabulary size with the ability to handle rare and out-of-vocabulary words effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implimentation: \n",
    "\n",
    "_Corpus: low low lower newer_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('l', 'o')\n",
      "{'lo w': 2, 'lo w e r': 1, 'n e w e r': 1, 'lo w e s t': 1}\n",
      "Merge 2: ('lo', 'w')\n",
      "{'low': 2, 'low e r': 1, 'n e w e r': 1, 'low e s t': 1}\n",
      "Merge 3: ('low', 'e')\n",
      "{'low': 2, 'lowe r': 1, 'n e w e r': 1, 'lowe s t': 1}\n",
      "Merge 4: ('lowe', 'r')\n",
      "{'low': 2, 'lower': 1, 'n e w e r': 1, 'lowe s t': 1}\n",
      "Merge 5: ('n', 'e')\n",
      "{'low': 2, 'lower': 1, 'ne w e r': 1, 'lowe s t': 1}\n",
      "Merge 6: ('ne', 'w')\n",
      "{'low': 2, 'lower': 1, 'new e r': 1, 'lowe s t': 1}\n",
      "Merge 7: ('new', 'e')\n",
      "{'low': 2, 'lower': 1, 'newe r': 1, 'lowe s t': 1}\n",
      "Merge 8: ('newe', 'r')\n",
      "{'low': 2, 'lower': 1, 'newer': 1, 'lowe s t': 1}\n",
      "Merge 9: ('lowe', 's')\n",
      "{'low': 2, 'lower': 1, 'newer': 1, 'lowes t': 1}\n",
      "Merge 10: ('lowes', 't')\n",
      "{'low': 2, 'lower': 1, 'newer': 1, 'lowest': 1}\n",
      "{'low': 2, 'lower': 1, 'newer': 1, 'lowest': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in v_in:\n",
    "        w_out = word.replace(bigram, replacement)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "# Example vocabulary\n",
    "vocab = {\n",
    "    'l o w': 2,\n",
    "    'l o w e r': 1,\n",
    "    'n e w e r': 1,\n",
    "    'l o w e s t': 1\n",
    "}\n",
    "\n",
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f'Merge {i+1}: {best}')\n",
    "    print(vocab)\n",
    "\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Normalization, Lemmatization and Stemming\n",
    "\n",
    "**_\"Word normalization is the task of putting words/tokens in a standard format.\"_**\n",
    "\n",
    "The case folding simplest case of word normalization is case folding. Mapping everything to lower\n",
    "case means that Woodchuck and woodchuck are represented identically, which is\n",
    "very helpful for generalization in many tasks, such as information retrieval or speech\n",
    "recognition.\n",
    "\n",
    "Systems that use BPE or other kinds of bottom-up tokenization may do no further word normalization. In other NLP systems, we may want to do further normalizations, like choosing a single normal form for words with multiple forms like\n",
    "USA and US or uh-huh and uhhuh.\n",
    "\n",
    "#### Lemmatization\n",
    "\n",
    "**_\"Lemmatization is the task of determining that two words have the same root, despite their surface differences.\"_**\n",
    "\n",
    "The words am, are, and is have the shared lemma be; the words dinner and dinners\n",
    "both have the lemma dinner. Lemmatizing each of these forms to the same lemma\n",
    "will let us find all mentions of words in Polish like Warsaw. The lemmatized form\n",
    "of a sentence like He is reading detective stories would thus be He be read detective\n",
    "story.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The         Lemma: the\n",
      "Token: children         Lemma: child\n",
      "Token: were         Lemma: be\n",
      "Token: running         Lemma: run\n",
      "Token: and         Lemma: and\n",
      "Token: had         Lemma: have\n",
      "Token: eaten         Lemma: eat\n",
      "Token: their         Lemma: their\n",
      "Token: meals         Lemma: meal\n",
      "Token: .         Lemma: .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"The children were running and had eaten their meals.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print tokens and their lemmas\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}         Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Lemmatization algorithms can be complex. For this reason we sometimes make\n",
    "use of a simpler but cruder method, which mainly consists of chopping off word final affixes. This naive version of morphological analysis is called stemming.\n",
    "\n",
    "_\"This was not the map we found in Billy Bones’s chest, but\n",
    "an accurate copy, complete in all things-names and heights\n",
    "and soundings-with the single exception of the red crosses\n",
    "and the written notes.\"_\n",
    "\n",
    "produces the following stemmed output:\n",
    "\n",
    "_\"Thi wa not the map we found in Billi Bone s chest but an\n",
    "accur copi complet in all thing name and height and sound\n",
    "with the singl except of the red cross and the written note\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The       Stem: the\n",
      "Token: children       Stem: children\n",
      "Token: were       Stem: were\n",
      "Token: running       Stem: run\n",
      "Token: and       Stem: and\n",
      "Token: had       Stem: had\n",
      "Token: eaten       Stem: eaten\n",
      "Token: their       Stem: their\n",
      "Token: meals       Stem: meal\n",
      "Token: .       Stem: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example text\n",
    "text = \"The children were running and had eaten their meals.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stems = [porter.stem(token) for token in tokens]\n",
    "\n",
    "# Print tokens and their stems\n",
    "for token, stem in zip(tokens, stems):\n",
    "    print(f\"Token: {token}       Stem: {stem}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sentence Segmentation\n",
    "\n",
    "Sentence segmentation is another important step in text processing. The most useful cues for segmenting a text into sentences are punctuation, like periods, question marks, and exclamation points. In general, sentence tokenization methods work by first deciding (based on rules\n",
    "or machine learning) whether a period is part of the word or is a sentence-boundary marker. An abbreviation dictionary can help determine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or machinelearned (Kiss and Strunk, 2006), as can the final sentence splitter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "How are you?\n",
      "I hope you're doing well.\n",
      "Have a great day!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"Hello! How are you? I hope you're doing well. Have a great day!\"\n",
    "\n",
    "# Segment the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Print each sentence\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords removal\n",
    "\n",
    "Stop word removal is a common preprocessing step in natural language processing (NLP) where commonly occurring words (e.g., \"the\", \"is\", \"in\") are removed from a text. These words are usually filtered out because they do not carry significant meaning and can reduce the efficiency of text processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'demonstrate', 'stop', 'word', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"This is a simple example to demonstrate stop word removal.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Remove stop words from the tokens\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "Regular expressions (regex) are a powerful tool in Python for pattern matching and text manipulation. The re module in Python provides support for regular expressions.\n",
    "\n",
    "Writing regular expression (regex) patterns can seem complex at first, but understanding the basic components and common patterns can make it much easier. Here's a guide to help you write regex patterns effectively:\n",
    "\n",
    "##### Basic Components of Regex\n",
    "- Character Classes\n",
    "    - [abc]         # Matches 'a', 'b', or 'c'\n",
    "    - [^abc]        # Matches any character except 'a', 'b', or 'c'\n",
    "    - [a-z]         # Matches any lowercase letter\n",
    "    - [A-Z]         # Matches any uppercase letter\n",
    "    - [0-9]         # Matches any digit\n",
    "    - \\d            # Matches any digit, equivalent to [0-9]\n",
    "    - \\D            # Matches any non-digit\n",
    "    - \\w            # Matches any alphanumeric character (word character)\n",
    "    - \\W            # Matches any non-alphanumeric character\n",
    "    - \\s            # Matches any whitespace character\n",
    "    - \\S            # Matches any non-whitespace character\n",
    "\n",
    "- Anchors\n",
    "    - ^             # Matches the start of the string\n",
    "    - $             # Matches the end of the string\n",
    "    - \\b            # Matches a word boundary\n",
    "    - \\B            # Matches a non-word boundary\n",
    "\n",
    "- Quantifiers\n",
    "    - \\*            # Matches 0 or more occurrences\n",
    "    - \\+            # Matches 1 or more occurrences\n",
    "    - ?             # Matches 0 or 1 occurrence\n",
    "    - {n}           # Matches exactly n occurrences\n",
    "    - {n,}          # Matches n or more occurrences\n",
    "    - {n,m}         # Matches between n and m occurrences\n",
    "\n",
    "- Grouping or Alteration\n",
    "    - (abc)         # Groups 'abc' together\n",
    "    - (a|b)         # Matches 'a' or 'b'\n",
    "\n",
    "##### Examples\n",
    "1. Email ID: [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\n",
    "2. Phone Number: \\d{3}[-.]\\d{3}[-.]\\d{4}\n",
    "3. URL: https?:\\/\\/(www\\.)?[a-zA-Z0-9._%+-]+\\.[a-zA-Z]{2,}\n",
    "4. Date: \\b\\d{2}[-/]\\d{2}[-/]\\d{4}\\b\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re.match() function checks if the beginning of a string matches the regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: hello\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'hello'\n",
    "text = 'hello world'\n",
    "\n",
    "match = re.match(pattern, text)\n",
    "if match:\n",
    "    print('Match found:', match.group())\n",
    "else:\n",
    "    print('No match')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re.search() function searches the entire string for a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search found: world\n"
     ]
    }
   ],
   "source": [
    "pattern = r'world'\n",
    "text = 'hello world'\n",
    "\n",
    "search = re.search(pattern, text)\n",
    "if search:\n",
    "    print('Search found:', search.group())\n",
    "else:\n",
    "    print('No match')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re.findall() function returns a list of all matches in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All matches: ['123', '456']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\d+'\n",
    "text = 'There are 123 apples and 456 oranges.'\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "print('All matches:', matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re.split() function splits the string by occurrences of the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: ['Split', 'this', 'text', 'into', 'words.']\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\s+'\n",
    "text = 'Split this text into words.'\n",
    "\n",
    "splits = re.split(pattern, text)\n",
    "print('Splits:', splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The re.sub() function replaces occurrences of the pattern with a replacement string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced text: I have oranges and oranges.\n"
     ]
    }
   ],
   "source": [
    "pattern = r'apples'\n",
    "text = 'I have apples and apples.'\n",
    "replacement = 'oranges'\n",
    "\n",
    "new_text = re.sub(pattern, replacement, text)\n",
    "print('Replaced text:', new_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_implmnt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
